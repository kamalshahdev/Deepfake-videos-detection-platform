"""Evaluate a trained checkpoint using cached multimodal features."""

from __future__ import annotations

import argparse
import json
from pathlib import Path
import sys

import numpy as np
from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support, roc_auc_score
import torch

ROOT_DIR = Path(__file__).resolve().parents[1]
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

from backend.app.services.model import MultimodalClassifier


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Evaluate checkpoint metrics")
    parser.add_argument("--checkpoint", type=Path, required=True)
    parser.add_argument("--feature-cache", type=Path, required=True, help="NPZ generated by training cache")
    parser.add_argument(
        "--split",
        choices=["val-from-checkpoint", "train-from-checkpoint", "all"],
        default="val-from-checkpoint",
    )
    parser.add_argument("--threshold", type=float, default=0.5)
    parser.add_argument("--min-auc", type=float, default=0.80)
    parser.add_argument("--min-fake-recall", type=float, default=0.75)
    parser.add_argument("--output-json", type=Path, default=None)
    return parser.parse_args()


def normalize(x: np.ndarray, mean: list[float], std: list[float]) -> np.ndarray:
    mean_np = np.asarray(mean, dtype=np.float32)
    std_np = np.asarray(std, dtype=np.float32) + 1e-6
    return (x - mean_np) / std_np


def resolve_indices(split: str, checkpoint: dict) -> np.ndarray:
    if split == "all":
        return np.asarray([], dtype=np.int64)

    key = "val_indices" if split == "val-from-checkpoint" else "train_indices"
    values = checkpoint.get(key)
    if values is None:
        raise ValueError(f"Checkpoint does not contain {key}")
    return np.asarray(values, dtype=np.int64)


def main() -> None:
    args = parse_args()

    if not args.checkpoint.exists():
        raise FileNotFoundError(f"Checkpoint missing: {args.checkpoint}")
    if not args.feature_cache.exists():
        raise FileNotFoundError(f"Feature cache missing: {args.feature_cache}")

    checkpoint = torch.load(args.checkpoint, map_location="cpu")
    cache = np.load(args.feature_cache)

    required = {"video", "audio", "metadata", "mask", "labels"}
    missing = required.difference(cache.files)
    if missing:
        raise ValueError(f"Feature cache missing arrays: {sorted(missing)}")

    video = np.asarray(cache["video"], dtype=np.float32)
    audio = np.asarray(cache["audio"], dtype=np.float32)
    meta = np.asarray(cache["metadata"], dtype=np.float32)
    mask = np.asarray(cache["mask"], dtype=np.float32)
    labels = np.asarray(cache["labels"], dtype=np.float32)

    norm = checkpoint.get("normalization", {})
    video = normalize(video, norm["video_mean"], norm["video_std"])
    audio = normalize(audio, norm["audio_mean"], norm["audio_std"])
    meta = normalize(meta, norm["meta_mean"], norm["meta_std"])

    indices = resolve_indices(args.split, checkpoint)
    if args.split == "all":
        x_video = video
        x_audio = audio
        x_meta = meta
        x_mask = mask
        y_true = labels
    else:
        x_video = video[indices]
        x_audio = audio[indices]
        x_meta = meta[indices]
        x_mask = mask[indices]
        y_true = labels[indices]

    model = MultimodalClassifier(
        video_dim=int(checkpoint["video_dim"]),
        audio_dim=int(checkpoint["audio_dim"]),
        meta_dim=int(checkpoint["meta_dim"]),
        hidden_dim=int(checkpoint["hidden_dim"]),
        dropout=float(checkpoint["dropout"]),
    )
    model.load_state_dict(checkpoint["model_state_dict"])
    model.eval()

    with torch.no_grad():
        logits, _ = model(
            torch.tensor(x_video, dtype=torch.float32),
            torch.tensor(x_audio, dtype=torch.float32),
            torch.tensor(x_meta, dtype=torch.float32),
            torch.tensor(x_mask, dtype=torch.float32),
        )
        probs = torch.sigmoid(logits).numpy()

    y_pred = (probs >= float(args.threshold)).astype(np.float32)

    accuracy = float(accuracy_score(y_true, y_pred))
    auc = float(roc_auc_score(y_true, probs)) if len(set(y_true.tolist())) > 1 else float("nan")
    precision, recall, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="binary", zero_division=0
    )
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()

    decision_go = (
        (not np.isnan(auc))
        and auc >= float(args.min_auc)
        and float(recall) >= float(args.min_fake_recall)
    )

    result = {
        "checkpoint": str(args.checkpoint),
        "feature_cache": str(args.feature_cache),
        "split": args.split,
        "threshold": float(args.threshold),
        "samples": int(len(y_true)),
        "accuracy": accuracy,
        "auc": auc,
        "precision_fake": float(precision),
        "recall_fake": float(recall),
        "f1_fake": float(f1),
        "confusion_matrix": {
            "tn": int(tn),
            "fp": int(fp),
            "fn": int(fn),
            "tp": int(tp),
        },
        "criteria": {
            "min_auc": float(args.min_auc),
            "min_fake_recall": float(args.min_fake_recall),
        },
        "decision": "go" if decision_go else "no-go",
    }

    print(json.dumps(result, indent=2))

    if args.output_json is not None:
        args.output_json.parent.mkdir(parents=True, exist_ok=True)
        args.output_json.write_text(json.dumps(result, indent=2), encoding="utf-8")
        print(f"Saved report: {args.output_json}")


if __name__ == "__main__":
    main()
